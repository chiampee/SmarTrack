/**
 * /api/enrich
 * ------------
 * Serverless edge function (Vercel) that receives raw page text and returns:
 *   • A concise TL;DR (3-sentence) summary generated by GPT-3.5-16k
 *   • An embedding vector for each ~800-token chunk (OpenAI embeddings v3-small)
 *
 * If `SUMMARY_BUCKET` and AWS creds are provided the result is also persisted to S3.
 * All errors produce JSON { error } responses.
 */
import type { VercelRequest, VercelResponse } from '@vercel/node';
import OpenAI from 'openai';
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const s3Enabled = !!process.env.SUMMARY_BUCKET;
let s3: S3Client | null = null;
if (s3Enabled) {
  s3 = new S3Client({
    region: process.env.AWS_REGION || 'us-east-1',
    credentials: process.env.AWS_ACCESS_KEY_ID
      ? {
          accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
          secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
        }
      : undefined,
  });
}

interface Chunk {
  id: number;
  text: string;
}

function chunkText(text: string, maxTokens = 800): Chunk[] {
  const approxChars = maxTokens * 4; // rough heuristic
  const parts: Chunk[] = [];
  for (let i = 0; i < text.length; i += approxChars) {
    parts.push({ id: parts.length, text: text.slice(i, i + approxChars) });
  }
  return parts;
}

const SUMMARY_MODEL = process.env.SUMMARY_MODEL || 'gpt-3.5-turbo-16k';
const EMB_MODEL = process.env.EMBED_MODEL || 'text-embedding-3-small';
const EMB_CONCURRENCY = parseInt(process.env.EMBED_CONCURRENCY || '5', 10);

function mapWithConcurrency<T, R>(items: T[], limit: number, fn: (item: T, idx: number) => Promise<R>): Promise<R[]> {
  const results: R[] = new Array(items.length);
  let idx = 0;
  const workers = Array(Math.min(limit, items.length)).fill(null).map(async () => {
    while (true) {
      const current = idx++;
      if (current >= items.length) break;
      results[current] = await fn(items[current], current);
    }
  });
  return Promise.all(workers).then(() => results);
}

export default async function handler(req: VercelRequest, res: VercelResponse) {
  if (req.method !== 'POST') {
    res.status(405).json({ error: 'Method not allowed' });
    return;
  }

  const MAX_INPUT_CHARS = 500_000; // align with client cap
  const { text = '', url = '' } = req.body as { text?: string; url?: string };
  if (!text || text.length < 100) {
    return res.status(400).json({ error: 'Text must be at least 100 characters' });
  }
  if (text.length > MAX_INPUT_CHARS) {
    return res.status(400).json({ error: `Text exceeds ${MAX_INPUT_CHARS} char limit` });
  }
  if (!process.env.OPENAI_API_KEY) {
    return res.status(500).json({ error: 'OPENAI_API_KEY not configured' });
  }

  try {
    const startTime = Date.now();
    // 1. TLDR summary
    const summaryResp = await openai.chat.completions.create({
      model: SUMMARY_MODEL,
      messages: [
        { role: 'system', content: 'Summarise the following web page in 3 concise sentences.' },
        { role: 'user', content: text.slice(0, 8000) },
      ],
      max_tokens: 150,
      temperature: 0.5,
    });
    const summary = summaryResp.choices[0].message?.content?.trim() || '';

    // 2. Embeddings per chunk (parallel with concurrency limit)
    const chunks = chunkText(text);
    const embeddings: number[][] = await mapWithConcurrency(chunks, EMB_CONCURRENCY, async (chunk) => {
      const emb = await openai.embeddings.create({
        model: EMB_MODEL,
        input: chunk.text,
      });
      return emb.data[0].embedding;
    });

    // Persist to remote storage if enabled
    if (s3Enabled && s3) {
      const Key = `${Date.now()}-${Math.random().toString(36).slice(2)}.json`;
      const Body = JSON.stringify({ url, summary, embeddings });
      try {
        await s3.send(
          new PutObjectCommand({
            Bucket: process.env.SUMMARY_BUCKET!,
            Key,
            Body,
            ContentType: 'application/json',
          }),
        );
      } catch (err) {
        console.error('S3 put error', err);
      }
    }

    console.info(`[enrich] url=${url || 'n/a'} processed in ${Date.now() - startTime}ms`);
    res.status(200).json({ summary, embeddings, embedding_model: EMB_MODEL });
  } catch (err: any) {
    console.error('Enrich error', url || 'n/a', err);
    const msg = err?.message || 'Unknown error';
    res.status(500).json({ error: msg });
  }
} 